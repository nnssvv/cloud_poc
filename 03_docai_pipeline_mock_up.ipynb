{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Libraires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1715779553489
        }
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient, Input\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml import load_component\n",
        "from azureml.core import Dataset, Datastore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Check credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1715769657086
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Get workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1715780447922
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found the config file in: ./config.json\n"
          ]
        }
      ],
      "source": [
        "ml_client = MLClient.from_config(\"config.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1715780449142
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Workspace\n",
        "ws = Workspace.from_config(\"config.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Check datastore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715769668358
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "datastore = ws.get_default_datastore()\n",
        "datastore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Add files from blob to datastore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1715769677970
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create the file dataset using the paths\n",
        "file_dataset = Dataset.File.from_files(path=datastore_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Data update step in pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1715780460583
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Workspace, Dataset, Run\n",
        "import os\n",
        "\n",
        "def get_workspace():\n",
        "    # Try to get the workspace from the run context if online\n",
        "    try:\n",
        "        run = Run.get_context(allow_offline=False)  # This throws an Exception if offline\n",
        "        return run.experiment.workspace\n",
        "    except Exception:\n",
        "        # If offline, load the workspace from the config file\n",
        "        return Workspace.from_config(\"config.json\")\n",
        "\n",
        "def update_or_create_pdf_dataset():\n",
        "    ws = get_workspace()\n",
        "    \n",
        "    # Access the existing datastore by its name\n",
        "    datastore = ws.datastores['input_pdfs']\n",
        "    \n",
        "    # Define the path pattern to include all PDF files\n",
        "    path_on_datastore = (datastore, '**/*.pdf')\n",
        "    \n",
        "    # Create or update the dataset\n",
        "    try:\n",
        "        # Create a FileDataset pointing to PDF files in the datastore\n",
        "        pdf_dataset = Dataset.File.from_files(path=path_on_datastore)\n",
        "        \n",
        "        # Register the dataset in the workspace for future use\n",
        "        pdf_dataset.register(workspace=ws,\n",
        "                             name='PDF File Dataset',\n",
        "                             description='Dataset containing all PDF files from the input_pdfs datastore',\n",
        "                             create_new_version=True)\n",
        "        print(\"Dataset updated or created successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to create or update the dataset: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1715770662203
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset updated or created successfully.\n"
          ]
        }
      ],
      "source": [
        "update_or_create_pdf_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Make it a step in the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1715782135411
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import ScriptRunConfig, Experiment, Environment\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "from azureml.pipeline.core import Pipeline\n",
        "from azureml.core.runconfig import RunConfiguration\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.pipeline.core import PipelineData\n",
        "\n",
        "# Load your Azure ML Workspace\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "# Optional: Define a custom run environment\n",
        "env = Environment(name=\"pdf_processing_environment\")\n",
        "conda_dep = CondaDependencies()\n",
        "conda_dep.add_pip_package(\"azureml-sdk\")  # Ensure the SDK is included\n",
        "env.python.conda_dependencies = conda_dep\n",
        "\n",
        "# Create a run configuration\n",
        "run_config = RunConfiguration()\n",
        "run_config.environment = env\n",
        "\n",
        "# Define the Python script step\n",
        "update_pdf_step = PythonScriptStep(\n",
        "    name='Update PDF Dataset',\n",
        "    script_name='update_pdf_dataset.py',  # Your script file name\n",
        "    arguments=[],  # Add script arguments here\n",
        "    compute_target='test-compute-ns',  # Specify the Azure ML compute target\n",
        "    runconfig=run_config,\n",
        "    source_directory='src/',  # Directory containing your script\n",
        "    allow_reuse=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1715783472002
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import ScriptRunConfig, Experiment, Environment, Workspace\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "from azureml.pipeline.core import Pipeline\n",
        "from azureml.core.runconfig import RunConfiguration\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.pipeline.core import PipelineData\n",
        "\n",
        "# Load your Azure ML Workspace\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "# Define a custom run environment\n",
        "env = Environment(name=\"pdf_processing_environment\")\n",
        "conda_dep = CondaDependencies()\n",
        "conda_dep.add_pip_package(\"azureml-sdk\")  # Ensure the SDK is included\n",
        "env.python.conda_dependencies = conda_dep\n",
        "\n",
        "# Create a run configuration\n",
        "run_config = RunConfiguration()\n",
        "run_config.environment = env\n",
        "\n",
        "# Define the output for the PDFs\n",
        "output_pdfs = PipelineData(name='output_pdfs', datastore=ws.get_default_datastore())\n",
        "\n",
        "# Define the Python script step\n",
        "update_pdf_step = PythonScriptStep(\n",
        "    name='Update PDF Dataset',\n",
        "    script_name='update_pdf_dataset.py',  # Your script file name\n",
        "    arguments=['--output_dir', output_pdfs],\n",
        "    outputs=[output_pdfs],\n",
        "    compute_target='test-compute-ns',  # Specify the Azure ML compute target\n",
        "    runconfig=run_config,\n",
        "    source_directory='src/',  # Directory containing your script\n",
        "    allow_reuse=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1715783503108
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create the pipeline\n",
        "pipeline = Pipeline(workspace=ws, steps=[update_pdf_step])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1715783511847
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created step Update PDF Dataset [52ab1003][329c318a-094b-4eff-b4ce-27018676e516], (This step will run and generate new outputs)\n",
            "Submitted PipelineRun f6347b2c-e3ca-4d72-adbc-3842d47f6ef6\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/f6347b2c-e3ca-4d72-adbc-3842d47f6ef6?wsid=/subscriptions/c85fd57e-ca5c-4866-9ae0-07accff31328/resourcegroups/rs-cloud-poc-ns/workspaces/aml-cloud-poc&tid=6be6e8b9-0525-4159-a288-e8c746abe0c3\n"
          ]
        }
      ],
      "source": [
        "# Create an experiment and run the pipeline\n",
        "experiment = Experiment(ws, 'Update_PDF_Dataset_Experiment')\n",
        "pipeline_run = experiment.submit(pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Add further steps to pipeline: Doc Ai custom model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1715779574240
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from azureml.core import Run, Dataset\n",
        "import configparser\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.ai.formrecognizer import DocumentAnalysisClient, AnalyzeResult\n",
        "from azure.core.serialization import AzureJSONEncoder\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1715779772962
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from azureml.core import Run, Dataset\n",
        "import configparser\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.ai.formrecognizer import DocumentAnalysisClient, AnalyzeResult\n",
        "from azure.core.serialization import AzureJSONEncoder\n",
        "import json\n",
        "\n",
        "# Set custom model and target form\n",
        "model_id = \"citi_test_model\"\n",
        "\n",
        "\n",
        "document_analysis_client = DocumentAnalysisClient(\n",
        "    endpoint=doc_endpoint, credential=AzureKeyCredential(doc_key)\n",
        ")\n",
        "\n",
        "# Run custom model\n",
        "model_id = \"citi_test_model\"\n",
        "form = open('test_docs/3Q23.pdf', 'rb')\n",
        "poller = document_analysis_client.begin_analyze_document(model_id, form)\n",
        "result = poller.result()\n",
        "\n",
        "# Result to dictionary\n",
        "result_dict = result.to_dict()\n",
        "\n",
        "with open(f'results.json' , 'w') as f:\n",
        "    json.dump(result_dict, f, cls = AzureJSONEncoder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1715780009981
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Result to dictionary\n",
        "result_dict = result.to_dict()\n",
        "\n",
        "with open(f'results.json' , 'w') as f:\n",
        "    json.dump(result_dict, f, cls = AzureJSONEncoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1715783995173
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import configparser\n",
        "\n",
        "# Load credentials\n",
        "config = configparser.ConfigParser(interpolation = None)\n",
        "config.read('config.ini')\n",
        "\n",
        "doc_endpoint = config['docintel']['endpoint']\n",
        "doc_key = config['docintel']['key']\n",
        "connection_str = config['storage']['connection_string']\n",
        "storage_key = config['storage']['key']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1715784029846
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import ScriptRunConfig, Experiment, Environment, Workspace\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "from azureml.pipeline.core import Pipeline\n",
        "from azureml.core.runconfig import RunConfiguration\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.pipeline.core import PipelineData\n",
        "\n",
        "# Load your Azure ML Workspace\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "# Optional: Define a custom run environment\n",
        "env = Environment(name=\"pdf_processing_environment\")\n",
        "conda_dep = CondaDependencies()\n",
        "conda_dep.add_pip_package(\"azureml-sdk\")  # Ensure the SDK is included\n",
        "conda_dep.add_pip_package(\"azure-ai-formrecognizer\")  # Ensure the Form Recognizer SDK is included\n",
        "env.python.conda_dependencies = conda_dep\n",
        "\n",
        "# Create a run configuration\n",
        "run_config = RunConfiguration()\n",
        "run_config.environment = env\n",
        "\n",
        "# Define the output for the PDFs\n",
        "output_pdfs = PipelineData(name='output_pdfs', datastore=ws.get_default_datastore())\n",
        "\n",
        "# Define the first step to update the PDF dataset\n",
        "update_pdf_step = PythonScriptStep(\n",
        "    name='Update PDF Dataset',\n",
        "    script_name='update_pdf_dataset.py',  # Your script file name\n",
        "    arguments=['--output_dir', output_pdfs],\n",
        "    outputs=[output_pdfs],\n",
        "    compute_target='test-compute-ns',  # Specify the Azure ML compute target\n",
        "    runconfig=run_config,\n",
        "    source_directory='src/',  # Directory containing your script\n",
        "    allow_reuse=True\n",
        ")\n",
        "\n",
        "# Define the output for the JSON files\n",
        "output_jsons = PipelineData(name='output_jsons', datastore=ws.get_default_datastore())\n",
        "\n",
        "# Define the second step to process the PDFs\n",
        "process_pdfs_step = PythonScriptStep(\n",
        "    name='Process PDFs',\n",
        "    script_name='process_pdfs.py',  # Your script file name\n",
        "    arguments=[\n",
        "        '--input_dir', output_pdfs,\n",
        "        '--output_dir', output_jsons,\n",
        "        '--doc_endpoint', doc_endpoint,  # Replace with your actual endpoint\n",
        "        '--doc_key', doc_key,  # Replace with your actual key\n",
        "    ],\n",
        "    inputs=[output_pdfs],\n",
        "    outputs=[output_jsons],\n",
        "    compute_target='test-compute-ns',  # Specify the Azure ML compute target\n",
        "    runconfig=run_config,\n",
        "    source_directory='src/',  # Directory containing your script\n",
        "    allow_reuse=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created step Update PDF Dataset [6a97e33b][ac57b0a4-5005-4993-bd56-f0dc1119fe69], (This step will run and generate new outputs)\n",
            "Created step Process PDFs [ec410c82][5953a45d-e169-4412-abcd-62b1837383cf], (This step will run and generate new outputs)\n",
            "Submitted PipelineRun 503441ba-6bdc-4f63-af96-350c4d723a51\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/503441ba-6bdc-4f63-af96-350c4d723a51?wsid=/subscriptions/c85fd57e-ca5c-4866-9ae0-07accff31328/resourcegroups/rs-cloud-poc-ns/workspaces/aml-cloud-poc&tid=6be6e8b9-0525-4159-a288-e8c746abe0c3\n",
            "PipelineRunId: 503441ba-6bdc-4f63-af96-350c4d723a51\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/503441ba-6bdc-4f63-af96-350c4d723a51?wsid=/subscriptions/c85fd57e-ca5c-4866-9ae0-07accff31328/resourcegroups/rs-cloud-poc-ns/workspaces/aml-cloud-poc&tid=6be6e8b9-0525-4159-a288-e8c746abe0c3\n",
            "PipelineRun Status: NotStarted\n",
            "PipelineRun Status: Running\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.\n",
            "This usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\n",
            "Please check for package conflicts in your python environment\n"
          ]
        }
      ],
      "source": [
        "# Create the pipeline with the steps\n",
        "pipeline = Pipeline(workspace=ws, steps=[update_pdf_step, process_pdfs_step])\n",
        "\n",
        "# Submit the pipeline\n",
        "experiment = Experiment(ws, 'pdf_processing_pipeline')\n",
        "pipeline_run = experiment.submit(pipeline)\n",
        "\n",
        "pipeline_run.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
